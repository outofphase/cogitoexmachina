{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"cogito ex machina Investigations into Artificial General Intelligence and related topics, especially philosophical questions about the limits of machine intelligence and the consequences of a possible intelligence explosion. cogito ex machina has two main sections: Opinion, contains my own thoughts on various AI topics an occasional blog of sorts Research, which contains reviews and critiques of books, articles and videos. For more information see About cogito ex machina . Coherent arguments about the existential risk of Artificial Intelligence 1st January 2019 I wanted to call this \" Steven Pinker was sent by Skynet to fool us! \" but the joke might have backfired In an October 2018 interview the Harvard academic and author Steven Pinker said that: he finds arguments about the existential threat of AI to be incoherent and that we should 'trust the engineers'. When asked about a tweet by Elon Musk about the difference between narrow AI and AGI, Steven Pinker waved it away, implied that Elon Musk is a hot-head and said that this was not the last time that \"Elon Musk has fired off an intemperate tweet\" . Although that may be true, it doesn't prove Pinker's point about AI not being worth spending too much time worrying about, so this short animated video examines Pinker's assertions, coolly. TODO: video review TODO: slides contra Pinker TODO: commentary contra Pinker Welcome to cogito ex machina 30th December 2018 The Opinion section is a blog of sorts, sharing provisional conclusions arising from my study and putting forward some of my own opinions. There is nothing much here yet because I wanted to get the piece about Steven Pinker's mistaken argument out quickly, but there is much more being worked on and that will follow later.","title":"Home"},{"location":"#cogito-ex-machina","text":"Investigations into Artificial General Intelligence and related topics, especially philosophical questions about the limits of machine intelligence and the consequences of a possible intelligence explosion. cogito ex machina has two main sections: Opinion, contains my own thoughts on various AI topics an occasional blog of sorts Research, which contains reviews and critiques of books, articles and videos. For more information see About cogito ex machina .","title":"cogito ex machina"},{"location":"#coherent-arguments-about-the-existential-risk-of-artificial-intelligence","text":"1st January 2019 I wanted to call this \" Steven Pinker was sent by Skynet to fool us! \" but the joke might have backfired In an October 2018 interview the Harvard academic and author Steven Pinker said that: he finds arguments about the existential threat of AI to be incoherent and that we should 'trust the engineers'. When asked about a tweet by Elon Musk about the difference between narrow AI and AGI, Steven Pinker waved it away, implied that Elon Musk is a hot-head and said that this was not the last time that \"Elon Musk has fired off an intemperate tweet\" . Although that may be true, it doesn't prove Pinker's point about AI not being worth spending too much time worrying about, so this short animated video examines Pinker's assertions, coolly. TODO: video review TODO: slides contra Pinker TODO: commentary contra Pinker","title":"Coherent arguments about the existential risk of Artificial Intelligence"},{"location":"#welcome-to-cogito-ex-machina","text":"30th December 2018 The Opinion section is a blog of sorts, sharing provisional conclusions arising from my study and putting forward some of my own opinions. There is nothing much here yet because I wanted to get the piece about Steven Pinker's mistaken argument out quickly, but there is much more being worked on and that will follow later.","title":"Welcome to cogito ex machina"},{"location":"about/","text":"About cogito ex machina Who's behind this? I'm David Morgan, I live in the UK and have interests in Computing, Artificial Intelligence and Philosophy, especially the Philosophy of Mind and the Problem of Consciousness, which is all sharpened by the prospect of an upcoming Singularity or Intelligence Explosion . I obtained my BA in Philosophy and Literature from Warwick University and my professional career has been in Information Technology, as a programmer, business analyst and product manager. Philosophical labels can be difficult things, and I don't know yet what I really think about these issues, but I can say that I'm not a Cartesian Dualist and also that I see the emergence of machine intelligence and even machine consciousness as possible in principle. What does the name cogito ex machina mean? For me it means 'thinking from a machine'. The words are Latin, although the phrase itself may not be grammatically-correct Latin. cogito indicates thinking or reasoning , as in Descartes' famous conclusion: cogito ergo sum , which is generally translated as I think therefore I am . ex machina indicates from the machine . It is the title of a beautiful film that explores themes of intelligence, consciousness and human values in robots. It's also a fragment from the Latin phrase deus ex machina , literally 'a god from the machine', which can be used in philosophy to refer to something metaphysical or outside the world of cause and effect, but ex machina here doesn't imply dualism , rather it echoes Gilbert Ryle's dismissal of dualism as imagining a 'ghost in the machine'. So in the end the site name cogito ex machina is just a mash-up, snagging several strands of philosophy and culture. How do I post comments? The site is static HTML with no Database or CMS behind it, so posting comments directly on the site is not possible, however you can open issues on the GitHub project where I maintain the site. If you want to contribute content then please fork the GitHub repository and submit a pull request. What's the Copyright? All content on this site is Copyright (c) 2019 by David Morgan and may be used under the terms of the CC-BY license: cogito ex machina by David Morgan is licensed under a Creative Commons Attribution 4.0 International License .","title":"About"},{"location":"about/#about-cogito-ex-machina","text":"","title":"About cogito ex machina"},{"location":"about/#whos-behind-this","text":"I'm David Morgan, I live in the UK and have interests in Computing, Artificial Intelligence and Philosophy, especially the Philosophy of Mind and the Problem of Consciousness, which is all sharpened by the prospect of an upcoming Singularity or Intelligence Explosion . I obtained my BA in Philosophy and Literature from Warwick University and my professional career has been in Information Technology, as a programmer, business analyst and product manager. Philosophical labels can be difficult things, and I don't know yet what I really think about these issues, but I can say that I'm not a Cartesian Dualist and also that I see the emergence of machine intelligence and even machine consciousness as possible in principle.","title":"Who's behind this?"},{"location":"about/#what-does-the-name-cogito-ex-machina-mean","text":"For me it means 'thinking from a machine'. The words are Latin, although the phrase itself may not be grammatically-correct Latin. cogito indicates thinking or reasoning , as in Descartes' famous conclusion: cogito ergo sum , which is generally translated as I think therefore I am . ex machina indicates from the machine . It is the title of a beautiful film that explores themes of intelligence, consciousness and human values in robots. It's also a fragment from the Latin phrase deus ex machina , literally 'a god from the machine', which can be used in philosophy to refer to something metaphysical or outside the world of cause and effect, but ex machina here doesn't imply dualism , rather it echoes Gilbert Ryle's dismissal of dualism as imagining a 'ghost in the machine'. So in the end the site name cogito ex machina is just a mash-up, snagging several strands of philosophy and culture.","title":"What does the name cogito ex machina mean?"},{"location":"about/#how-do-i-post-comments","text":"The site is static HTML with no Database or CMS behind it, so posting comments directly on the site is not possible, however you can open issues on the GitHub project where I maintain the site. If you want to contribute content then please fork the GitHub repository and submit a pull request.","title":"How do I post comments?"},{"location":"about/#whats-the-copyright","text":"All content on this site is Copyright (c) 2019 by David Morgan and may be used under the terms of the CC-BY license: cogito ex machina by David Morgan is licensed under a Creative Commons Attribution 4.0 International License .","title":"What's the Copyright?"},{"location":"biblio/","text":"FHIOxford. AI Toy Control Problem. Accessed January 6, 2019. https://www.youtube.com/watch?v=sx8JkdbNgdU feature=youtu.be. Bostrom, Nick. \u201cSuperintelligence: Paths, Dangers, Strategies,\u201d 2014. Lex Fridman. MIT AI: AI in the Age of Reason (Steven Pinker). Accessed January 6, 2019. https://www.youtube.com/watch?v=epQxfSp-rdU feature=youtu.be. Pinker, Steven. Enlightenment Now: The Case for Reason, Science, Humanism and Progress. London: Allen Lane, an imprint of Penguin Books, 2018.","title":"References"},{"location":"sites/","text":"Future of Humanity Institute FHI is a research group investigating various big issues for humanity, including existential risks such as might be posed by AI. LessWrong LESSWRONG is a community blog that discusses human rationality. MIT 6.S099: Artificial General Intelligence MIT 6.S099 is a series of lectures and interviews given by big name speakers around the topic of AGI.","title":"Links"},{"location":"sites/#future-of-humanity-institute","text":"FHI is a research group investigating various big issues for humanity, including existential risks such as might be posed by AI.","title":"Future of Humanity Institute"},{"location":"sites/#lesswrong","text":"LESSWRONG is a community blog that discusses human rationality.","title":"LessWrong"},{"location":"sites/#mit-6s099-artificial-general-intelligence","text":"MIT 6.S099 is a series of lectures and interviews given by big name speakers around the topic of AGI.","title":"MIT 6.S099: Artificial General Intelligence"},{"location":"opinion/20181230_first_post/","text":"Welcome to cogito ex machina 30th December 2018 The Opinion section is a blog of sorts, sharing provisional conclusions arising from my study and putting forward some of my own opinions. There is nothing much here yet because I wanted to get the piece about Steven Pinker's mistaken argument out quickly, but there is much more being worked on and that will follow later.","title":"20181230 first post"},{"location":"opinion/20181230_first_post/#welcome-to-cogito-ex-machina","text":"30th December 2018 The Opinion section is a blog of sorts, sharing provisional conclusions arising from my study and putting forward some of my own opinions. There is nothing much here yet because I wanted to get the piece about Steven Pinker's mistaken argument out quickly, but there is much more being worked on and that will follow later.","title":"Welcome to cogito ex machina"},{"location":"opinion/20190101_pinker/","text":"Coherent arguments about the existential risk of Artificial Intelligence 1st January 2019 I wanted to call this \" Steven Pinker was sent by Skynet to fool us! \" but the joke might have backfired In an October 2018 interview the Harvard academic and author Steven Pinker said that: he finds arguments about the existential threat of AI to be incoherent and that we should 'trust the engineers'. When asked about a tweet by Elon Musk about the difference between narrow AI and AGI, Steven Pinker waved it away, implied that Elon Musk is a hot-head and said that this was not the last time that \"Elon Musk has fired off an intemperate tweet\" . Although that may be true, it doesn't prove Pinker's point about AI not being worth spending too much time worrying about, so this short animated video examines Pinker's assertions, coolly. TODO: video review TODO: slides contra Pinker TODO: commentary contra Pinker","title":"20190101 pinker"},{"location":"opinion/20190101_pinker/#coherent-arguments-about-the-existential-risk-of-artificial-intelligence","text":"1st January 2019 I wanted to call this \" Steven Pinker was sent by Skynet to fool us! \" but the joke might have backfired In an October 2018 interview the Harvard academic and author Steven Pinker said that: he finds arguments about the existential threat of AI to be incoherent and that we should 'trust the engineers'. When asked about a tweet by Elon Musk about the difference between narrow AI and AGI, Steven Pinker waved it away, implied that Elon Musk is a hot-head and said that this was not the last time that \"Elon Musk has fired off an intemperate tweet\" . Although that may be true, it doesn't prove Pinker's point about AI not being worth spending too much time worrying about, so this short animated video examines Pinker's assertions, coolly. TODO: video review TODO: slides contra Pinker TODO: commentary contra Pinker","title":"Coherent arguments about the existential risk of Artificial Intelligence"},{"location":"opinion/posts/","text":"Posts December 2018 Welcome to cogito ex machina 30th December 2018 The Opinion section is a blog of sorts, sharing provisional conclusions arising from my study and putting forward some of my own opinions. There is nothing much here yet because I wanted to get the piece about Steven Pinker's mistaken argument out quickly, but there is much more being worked on and that will follow later. January 2019 Coherent arguments about the existential risk of Artificial Intelligence 1st January 2019 I wanted to call this \" Steven Pinker was sent by Skynet to fool us! \" but the joke might have backfired In an October 2018 interview the Harvard academic and author Steven Pinker said that: he finds arguments about the existential threat of AI to be incoherent and that we should 'trust the engineers'. When asked about a tweet by Elon Musk about the difference between narrow AI and AGI, Steven Pinker waved it away, implied that Elon Musk is a hot-head and said that this was not the last time that \"Elon Musk has fired off an intemperate tweet\" . Although that may be true, it doesn't prove Pinker's point about AI not being worth spending too much time worrying about, so this short animated video examines Pinker's assertions, coolly. Read more...","title":"Opinion"},{"location":"opinion/posts/#posts","text":"","title":"Posts"},{"location":"opinion/posts/#december-2018","text":"","title":"December 2018"},{"location":"opinion/posts/#welcome-to-cogito-ex-machina","text":"30th December 2018 The Opinion section is a blog of sorts, sharing provisional conclusions arising from my study and putting forward some of my own opinions. There is nothing much here yet because I wanted to get the piece about Steven Pinker's mistaken argument out quickly, but there is much more being worked on and that will follow later.","title":"Welcome to cogito ex machina"},{"location":"opinion/posts/#january-2019","text":"","title":"January 2019"},{"location":"opinion/posts/#coherent-arguments-about-the-existential-risk-of-artificial-intelligence","text":"1st January 2019 I wanted to call this \" Steven Pinker was sent by Skynet to fool us! \" but the joke might have backfired In an October 2018 interview the Harvard academic and author Steven Pinker said that: he finds arguments about the existential threat of AI to be incoherent and that we should 'trust the engineers'. When asked about a tweet by Elon Musk about the difference between narrow AI and AGI, Steven Pinker waved it away, implied that Elon Musk is a hot-head and said that this was not the last time that \"Elon Musk has fired off an intemperate tweet\" . Although that may be true, it doesn't prove Pinker's point about AI not being worth spending too much time worrying about, so this short animated video examines Pinker's assertions, coolly. Read more...","title":"Coherent arguments about the existential risk of Artificial Intelligence"},{"location":"research/20181228_superintelligence_bostrom/","text":"Superintelligence by Nick Bostrom (2014) 28th December 2018 A comprehensive review the existential risk posed by Artificial Intelligence and in particular by the possibility of a rapid intelligence explosion , and strategies to mitigate this risk. Concludes that although the timing and form of Superintelligence is uncertain, the risks are real and argues that urgent research into various problems is needed to allow us to take precautionary steps. Originally published in 2014, the 2016 paperback edition includes an short Afterword which reviews some of the subsequent developments in this field. In broad outline the book examines: various possible types of AI the chance of a rapid take-off of Superintelligence, in the form of one or more AIs with reasoning and planning skills that are far in advance of humans the problem of controlling such an AI and the concept of an instrumental reason (i.e. a means to an end ) the difficulty of defining correct goals for a Superintelligence possible outcomes for humanity on different scenarios. Chapters 1-5 TBC Chapter 6 -- Cognitive superpowers Identifies six strategically relevant tasks for an advanced cognitive system and the skill-set that each task requires. These tasks are: Intelligence amplification Strategizing Social manipulation Hacking Technology research Economic productivity Argues that a sufficiently capable AI with one of these capabilities might conclude that gaining other capacities was a good 'means to an end', and that it might be able to leverage the first capability to acquire the others. Chapters 7-15 TBC","title":"20181228 superintelligence bostrom"},{"location":"research/20181228_superintelligence_bostrom/#superintelligence-by-nick-bostrom-2014","text":"28th December 2018 A comprehensive review the existential risk posed by Artificial Intelligence and in particular by the possibility of a rapid intelligence explosion , and strategies to mitigate this risk. Concludes that although the timing and form of Superintelligence is uncertain, the risks are real and argues that urgent research into various problems is needed to allow us to take precautionary steps. Originally published in 2014, the 2016 paperback edition includes an short Afterword which reviews some of the subsequent developments in this field. In broad outline the book examines: various possible types of AI the chance of a rapid take-off of Superintelligence, in the form of one or more AIs with reasoning and planning skills that are far in advance of humans the problem of controlling such an AI and the concept of an instrumental reason (i.e. a means to an end ) the difficulty of defining correct goals for a Superintelligence possible outcomes for humanity on different scenarios.","title":"Superintelligence by Nick Bostrom (2014)"},{"location":"research/20181228_superintelligence_bostrom/#chapters-1-5","text":"TBC","title":"Chapters 1-5"},{"location":"research/20181228_superintelligence_bostrom/#chapter-6-cognitive-superpowers","text":"Identifies six strategically relevant tasks for an advanced cognitive system and the skill-set that each task requires. These tasks are: Intelligence amplification Strategizing Social manipulation Hacking Technology research Economic productivity Argues that a sufficiently capable AI with one of these capabilities might conclude that gaining other capacities was a good 'means to an end', and that it might be able to leverage the first capability to acquire the others.","title":"Chapter 6 -- Cognitive superpowers"},{"location":"research/20181228_superintelligence_bostrom/#chapters-7-15","text":"TBC","title":"Chapters 7-15"},{"location":"research/20181229_armstrong_ai_toy_control/","text":"AI Toy Control Problem - Stuart Armstrong (2017) 29th December 2018 A short animated YouTube video that illustrates the AI Control problem by using a simple AI agent which has a goal that is slightly misaligned with its creator's goal. A key point is that this shows an AI with sufficient planning skills can discover deceitful behaviour which evades safety provisions. if this improves realisation of its goal, without there being any implication of self-awareness or of independent will in the agent. The ideas are a summary of a 2015 paper and discussion on LessWrong .","title":"20181229 armstrong ai toy control"},{"location":"research/20181229_armstrong_ai_toy_control/#ai-toy-control-problem-stuart-armstrong-2017","text":"29th December 2018 A short animated YouTube video that illustrates the AI Control problem by using a simple AI agent which has a goal that is slightly misaligned with its creator's goal. A key point is that this shows an AI with sufficient planning skills can discover deceitful behaviour which evades safety provisions. if this improves realisation of its goal, without there being any implication of self-awareness or of independent will in the agent. The ideas are a summary of a 2015 paper and discussion on LessWrong .","title":"AI Toy Control Problem - Stuart Armstrong (2017)"},{"location":"research/20181230_pinker_fridman/","text":"Interview by Lex Fridman - Steven Pinker (2018) 30th December 2018 Two kinds of existential threat have been discussed and both are incoherent: an AI take-over where a more advanced intelligence will take over..\u2026confuses intelligence with a will to power..\u2026humans have traits such as dominance but a pure problem solving systems won't have that as one of its goals..\u2026\"its goals will be whatever we set its goals as\" and there is no reason to suppose it will evolve in that direction and we can assume that we won't give it stupid goals such as maximising its own power..\u2026analogy between nuclear weapons and AI is fundamentally misguided since the whole point of nuclear weapons is to destroy things, which is not the goal of AI..\u2026goals are external and if we don't design an AI system to maximise dominance then it won't..\u2026we're just so familiar with homo sapiens that we'll be collateral damage such as a goal of maximising paperclips, curing cancer or world peace, each of which can have unwanted results (what Bostrom calls perverse instantiations ) which Steven Pinker says he finds \"utterly fanciful and in fact self-defeating\" because it assumes we are intelligent enough to engineer a superintelligence but not enough to give it a sensible goal, and that the system is so smart that it can 'cure cancer' but so idiotic that it can't figure out what we mean by 'curing cancer' also implies other goals..\u2026so the value alignment problem is based on a misconception. Furthermore all engineering systems are a trade-off between multiple goals and this is actually part of the definition of intelligence. And on the question of the timescale of the threat of AI as existential threat..\u2026it's fanciful..\u2026culture of engineering is to test systems before using them..\u2026assume magically powered intelligence that is connected to infrastructure without testing..\u2026need some legal framework perhaps for irresponsible engineers..\u2026but have never seen sufficiently credible argument / plausible scenario to spend much time worrying about it..\u2026same culture of safety that is part of engineering mindset should not be discarded..\u2026idea of a step-function, a recursive self-improvement, a foom is 'fanciful'..\u2026the thinking about a step-function is magical thinking that is not in line with reality of current AI. We should be thinking about pandemics, climate change, nuclear weapons instead. Sam Harris fingered the 'rationality' community as fatalists. Pinker says we need to prioritise between probable issues and imaginable but close to zero-probability ones, or this leads to fatalism or paralysis. Accepts human effort is 'not well calibrated against risk' because it is based on what is imaginable, compare for example terrorism vs road deaths. Need to calibrate our budget of fear planning to probability. Culture of engineering is to think about risks. Psychology of negativity is a phenomenon, part of our negativity bias that is built in to our species.","title":"20181230 pinker fridman"},{"location":"research/20181230_pinker_fridman/#interview-by-lex-fridman-steven-pinker-2018","text":"30th December 2018 Two kinds of existential threat have been discussed and both are incoherent: an AI take-over where a more advanced intelligence will take over..\u2026confuses intelligence with a will to power..\u2026humans have traits such as dominance but a pure problem solving systems won't have that as one of its goals..\u2026\"its goals will be whatever we set its goals as\" and there is no reason to suppose it will evolve in that direction and we can assume that we won't give it stupid goals such as maximising its own power..\u2026analogy between nuclear weapons and AI is fundamentally misguided since the whole point of nuclear weapons is to destroy things, which is not the goal of AI..\u2026goals are external and if we don't design an AI system to maximise dominance then it won't..\u2026we're just so familiar with homo sapiens that we'll be collateral damage such as a goal of maximising paperclips, curing cancer or world peace, each of which can have unwanted results (what Bostrom calls perverse instantiations ) which Steven Pinker says he finds \"utterly fanciful and in fact self-defeating\" because it assumes we are intelligent enough to engineer a superintelligence but not enough to give it a sensible goal, and that the system is so smart that it can 'cure cancer' but so idiotic that it can't figure out what we mean by 'curing cancer' also implies other goals..\u2026so the value alignment problem is based on a misconception. Furthermore all engineering systems are a trade-off between multiple goals and this is actually part of the definition of intelligence. And on the question of the timescale of the threat of AI as existential threat..\u2026it's fanciful..\u2026culture of engineering is to test systems before using them..\u2026assume magically powered intelligence that is connected to infrastructure without testing..\u2026need some legal framework perhaps for irresponsible engineers..\u2026but have never seen sufficiently credible argument / plausible scenario to spend much time worrying about it..\u2026same culture of safety that is part of engineering mindset should not be discarded..\u2026idea of a step-function, a recursive self-improvement, a foom is 'fanciful'..\u2026the thinking about a step-function is magical thinking that is not in line with reality of current AI. We should be thinking about pandemics, climate change, nuclear weapons instead. Sam Harris fingered the 'rationality' community as fatalists. Pinker says we need to prioritise between probable issues and imaginable but close to zero-probability ones, or this leads to fatalism or paralysis. Accepts human effort is 'not well calibrated against risk' because it is based on what is imaginable, compare for example terrorism vs road deaths. Need to calibrate our budget of fear planning to probability. Culture of engineering is to think about risks. Psychology of negativity is a phenomenon, part of our negativity bias that is built in to our species.","title":"Interview by Lex Fridman - Steven Pinker (2018)"},{"location":"research/20181231_enlightenment_now_pinker/","text":"Enlightenment Now by Steven Pinker (2018) The Case for Reason, Science, Humanism and Progress 31st December 2018 Presents a positive vision of a new Age of Enlightenment in which human intelligence and culture is shown to be improving the lot of humanity. This vision is presented as a counterpoint to what Pinker sees as all too prevalent intellectual pessimism, but sadly it ends up becoming smug, unenlightened sneering at other contemporary intellectual discovery. Chapters 1-18 TBC Chapter 19 -- Existential Threats Considers whether the positive signs discussed in earlier chapters are overshadowed by catastrophic or existential risks, meaning a class of threats which may have a low probability but an enormous negative impact. Pinker's stated aim here is not to deny that these risks exist, but to set out a framework in which to think about them. He sees this as necessary, since overemphasising major risks is at best a distraction from more immediate threats and at worst creates dangers in itself, his main example being the claim that nuclear proliferation occurred as a result of unwarranted fear. Pinker points out that human risk assessment is poor and is based on whether something is imaginable rather than whether it is probable. But, accepting that this is true, this tendency is just one example of built-in human cognitive bias, so this justification seems wide of the mark, since the main windmill that Pinker tilts at here is the 'rationality movement' and that group in fact stresses the need to be aware of and to use techniques to correct for many kinds of cognitive bias. Although his example of nuclear proliferation needs to be adjusted to take account of the political and economic forces that were the main motors of the post-war arms race, there is a real issue here too of fear caused by ignorance, but the solution to that is more openness . Perversely his argument, that the discussion of low probability risks should be curtailed because it might paralyse people, seems the opposite both of such openness and of the Enlightenment spirit itself. Turning to the danger of AI specifically, Pinker argues that this is based more our association of intelligence (the ability to reason and plan) with a will to power (seeking dominance and the pursuit of personal goals). Pinker argues that while systems that have evolved in survival of the fittest competition do sometimes have this combination, there is no good reason to suppose that a machine intelligence will have both. Whether that is true however seems to depend a lot on how that machine intelligence is arrived at: if is designed and programmed by software engineers then perhaps it is true that there need be no will to power , but if it is largely a system created using Machine Learning then there is less reason to think it can't happen, particularly if the system development was a hybrid of Genetic Algorithms and Machine Learning. And if machine intelligence is achieved by functional emulation of a human brain then there is no reason to believe that some unwanted evolutionary traits will not be carried over into the machine intelligence. An important point to make here is that the risk of powerful general-purpose AI doesn't require the AI to be self-aware, or conscious in the sense of having subjective feelings. We don't have to suppose it has its own ends. If an intelligent system vastly exceeds human capability then a large risk can come from even a small misalignment between the goals of the AI and the goals of its creator. This is known as the AI Control Problem , see for example Armstrong, 2017 . A second issue that Pinker identifies is about the nature of intelligence itself, which he defines as how to \"pursue various goals in various domains\", and he makes the point that the domains which are allocated to machine intelligences will not be the same domains that humans have evolved to deal with. As examples, he says that a machine intelligence might not need to solve problems like \"charming mates\", just as our human intelligence has not evolved to solve problems like \"sorting millions of accounting records\". Because there are so many different domains, and so little overlap between human and machine spheres of interest, he says the concept of Artificial General Intelligence itself \"is barely coherent\". Knowledge, he says, doesn't come from \"running an algorithm faster and faster\". There are numerous problems with this line of argument. First, he doesn't consider what goals the sponsor of an advanced AI might have, but some of the best-funded advanced AI initiatives are likely to be from nation-states with military objectives. Given such a sponsor, then problematic goals and inconsistent rules about how humans can be treated may be baked-in to the goals themselves, in which case running the algorithm faster and faster does create a problem if that means there is insufficient time for its human monitors to understand the strategies the AI has devised to achieve those goals. Secondly, once a sufficiently advanced machine intelligence has achieved considerable superiority to humans in one area of expertise, it may rapidly learn how to extend this superiority into other domains too, if it calculates that this will help it achieve its goals. This is the problem of the expansion of cognitive superpowers discussed by Bostrom, 2014 , who identified six different superpowers that a machine intelligence might have, any one of which might be used to bootstrap itself to achieve other superpowers if this furthered its goals. Overall then, despite the talk of different domains of intelligence, the problem here is still one of goals: who sets the goals, how the goals can be defined rigorously, how they are translated into targets for the AI and how to protect against interim sub-goals that may emerge. Once again here, an important point is that the Control Problem doesn't require that the AI is self-aware or conscious in the sense of having subjective experience. It may be sufficient if the AI places a high value on a particular final goal and it reasons that some indirect course of action is the best means to that end. It may then seek abilities in different domains simply as a by-product of pursing its final goal. The chapter then addresses the \"Value Alignment Problem\" and pokes fun at some scenarios raised by Bostrom. Pinker's argument is simplistic however, saying that if we are smart enough to build an AGI then we are smart enough to test it for safety, and his conclusion is trite: \"The way to deal with this threat [of a Doomsday Computer] is straightforward: don\u2019t build one\". This misses the point that even a tiny mistake in establishing the goals of a system will become a huge problem if that system is smarter and faster than you are. And mistakes do happen in software definition all the time. The most confusing thing about the overall thrust of Pinker's argument in this chapter is that on the one hand he derides 'sci-fi scaremongering' and 'scientific pessimism', while on the other fully accepting that some risks are real and also arguing that awareness of risks will allow intelligent progress towards risk-reduction. That is, the argument seems to be: there are genuine existential risks a subset of these can be exacerbated by ignorance and fear most risks can be reduced, by using scientific knowledge, intelligence and cooperation but an over-emphasis on existential risks leads (because of our innate predispositions and cognitive biases) to either fatalism or ineffective activism, both of which reduce the effort spent on achievable risk reduction in other areas such as climate change with the main enemy being the alarmists of the rationality movement. This simply makes no sense, as the rationality movement is itself an attempt to bring knowledge and intelligence to bear on issues, while using techniques from statistics and social sciences to compensate for innate predispositions and biases. In reaction to perceived alarmists, Pinker throws the baby out with the bathwater, seemingly because the bathwater wasn't just the right temperature, i.e. didn't strike the right balance between ignorance and over-sensitivity for his liking. Furthermore in order to make this argument he caricatures the rationality movement as alarmist, but examination of its methods and texts leads to the opposite conclusion: it doesn't say that \"humanity is screwed\", but on the contrary that a thoughtful and bias-aware investigation of risk-adjusted costs is needed, hopefully leading to informed and effective mitigation of those risks. It's also a lazy critique, in that Pinker pokes fun at the term \"existential risk\" but doesn't engage here with a central problem in this area: the problem of how to account for very small probabilities of extremely bad outcomes. The point of rationalist thought-experiments is to expand the frame of reference so that we can make better choices, but Pinker doesn't want any fresh perspectives that might pop his progress-bubble. The chapter finishes with a review of nuclear proliferation, and concludes that we should continue steady progress towards reducing warhead stockpiles and seek to extend the 'no first use' doctrine. There is nothing much to note about this, except that he doesn't show that progress on this front really requires us to abstain from investigation of other risks. Overall this chapter misses his mark because it misrepresents its opponents in several ways and it doesn't engage with the central problems: the rationality movement is in fact very aware of the danger of cognitive bias in decision-making the superintelligence argument is that we must think about precautions to take, rather than being an attempt to halt all progress towards AGI (beneficial AGI, if achieved, would be valuable in combating other existential risks) defining AI goals and aligning AI values to our values are hard problems that need informed and widespread discussion what is at stake is potentially of astronomical importance and so it's inappropriate to apply our instinctive risk-assessment heuristics to it. Despite his initial claim that existential threats are \"problems to be solved\", all that Pinker's review of the field ends up doing is to suggest that -- aside from the issue of nuclear warhead decommissioning -- these problems can be expected to solve themselves in the normal course of events. Chapters 19-23 TBC","title":"20181231 enlightenment now pinker"},{"location":"research/20181231_enlightenment_now_pinker/#enlightenment-now-by-steven-pinker-2018","text":"The Case for Reason, Science, Humanism and Progress 31st December 2018 Presents a positive vision of a new Age of Enlightenment in which human intelligence and culture is shown to be improving the lot of humanity. This vision is presented as a counterpoint to what Pinker sees as all too prevalent intellectual pessimism, but sadly it ends up becoming smug, unenlightened sneering at other contemporary intellectual discovery.","title":"Enlightenment Now by Steven Pinker (2018)"},{"location":"research/20181231_enlightenment_now_pinker/#chapters-1-18","text":"TBC","title":"Chapters 1-18"},{"location":"research/20181231_enlightenment_now_pinker/#chapter-19-existential-threats","text":"Considers whether the positive signs discussed in earlier chapters are overshadowed by catastrophic or existential risks, meaning a class of threats which may have a low probability but an enormous negative impact. Pinker's stated aim here is not to deny that these risks exist, but to set out a framework in which to think about them. He sees this as necessary, since overemphasising major risks is at best a distraction from more immediate threats and at worst creates dangers in itself, his main example being the claim that nuclear proliferation occurred as a result of unwarranted fear. Pinker points out that human risk assessment is poor and is based on whether something is imaginable rather than whether it is probable. But, accepting that this is true, this tendency is just one example of built-in human cognitive bias, so this justification seems wide of the mark, since the main windmill that Pinker tilts at here is the 'rationality movement' and that group in fact stresses the need to be aware of and to use techniques to correct for many kinds of cognitive bias. Although his example of nuclear proliferation needs to be adjusted to take account of the political and economic forces that were the main motors of the post-war arms race, there is a real issue here too of fear caused by ignorance, but the solution to that is more openness . Perversely his argument, that the discussion of low probability risks should be curtailed because it might paralyse people, seems the opposite both of such openness and of the Enlightenment spirit itself. Turning to the danger of AI specifically, Pinker argues that this is based more our association of intelligence (the ability to reason and plan) with a will to power (seeking dominance and the pursuit of personal goals). Pinker argues that while systems that have evolved in survival of the fittest competition do sometimes have this combination, there is no good reason to suppose that a machine intelligence will have both. Whether that is true however seems to depend a lot on how that machine intelligence is arrived at: if is designed and programmed by software engineers then perhaps it is true that there need be no will to power , but if it is largely a system created using Machine Learning then there is less reason to think it can't happen, particularly if the system development was a hybrid of Genetic Algorithms and Machine Learning. And if machine intelligence is achieved by functional emulation of a human brain then there is no reason to believe that some unwanted evolutionary traits will not be carried over into the machine intelligence. An important point to make here is that the risk of powerful general-purpose AI doesn't require the AI to be self-aware, or conscious in the sense of having subjective feelings. We don't have to suppose it has its own ends. If an intelligent system vastly exceeds human capability then a large risk can come from even a small misalignment between the goals of the AI and the goals of its creator. This is known as the AI Control Problem , see for example Armstrong, 2017 . A second issue that Pinker identifies is about the nature of intelligence itself, which he defines as how to \"pursue various goals in various domains\", and he makes the point that the domains which are allocated to machine intelligences will not be the same domains that humans have evolved to deal with. As examples, he says that a machine intelligence might not need to solve problems like \"charming mates\", just as our human intelligence has not evolved to solve problems like \"sorting millions of accounting records\". Because there are so many different domains, and so little overlap between human and machine spheres of interest, he says the concept of Artificial General Intelligence itself \"is barely coherent\". Knowledge, he says, doesn't come from \"running an algorithm faster and faster\". There are numerous problems with this line of argument. First, he doesn't consider what goals the sponsor of an advanced AI might have, but some of the best-funded advanced AI initiatives are likely to be from nation-states with military objectives. Given such a sponsor, then problematic goals and inconsistent rules about how humans can be treated may be baked-in to the goals themselves, in which case running the algorithm faster and faster does create a problem if that means there is insufficient time for its human monitors to understand the strategies the AI has devised to achieve those goals. Secondly, once a sufficiently advanced machine intelligence has achieved considerable superiority to humans in one area of expertise, it may rapidly learn how to extend this superiority into other domains too, if it calculates that this will help it achieve its goals. This is the problem of the expansion of cognitive superpowers discussed by Bostrom, 2014 , who identified six different superpowers that a machine intelligence might have, any one of which might be used to bootstrap itself to achieve other superpowers if this furthered its goals. Overall then, despite the talk of different domains of intelligence, the problem here is still one of goals: who sets the goals, how the goals can be defined rigorously, how they are translated into targets for the AI and how to protect against interim sub-goals that may emerge. Once again here, an important point is that the Control Problem doesn't require that the AI is self-aware or conscious in the sense of having subjective experience. It may be sufficient if the AI places a high value on a particular final goal and it reasons that some indirect course of action is the best means to that end. It may then seek abilities in different domains simply as a by-product of pursing its final goal. The chapter then addresses the \"Value Alignment Problem\" and pokes fun at some scenarios raised by Bostrom. Pinker's argument is simplistic however, saying that if we are smart enough to build an AGI then we are smart enough to test it for safety, and his conclusion is trite: \"The way to deal with this threat [of a Doomsday Computer] is straightforward: don\u2019t build one\". This misses the point that even a tiny mistake in establishing the goals of a system will become a huge problem if that system is smarter and faster than you are. And mistakes do happen in software definition all the time. The most confusing thing about the overall thrust of Pinker's argument in this chapter is that on the one hand he derides 'sci-fi scaremongering' and 'scientific pessimism', while on the other fully accepting that some risks are real and also arguing that awareness of risks will allow intelligent progress towards risk-reduction. That is, the argument seems to be: there are genuine existential risks a subset of these can be exacerbated by ignorance and fear most risks can be reduced, by using scientific knowledge, intelligence and cooperation but an over-emphasis on existential risks leads (because of our innate predispositions and cognitive biases) to either fatalism or ineffective activism, both of which reduce the effort spent on achievable risk reduction in other areas such as climate change with the main enemy being the alarmists of the rationality movement. This simply makes no sense, as the rationality movement is itself an attempt to bring knowledge and intelligence to bear on issues, while using techniques from statistics and social sciences to compensate for innate predispositions and biases. In reaction to perceived alarmists, Pinker throws the baby out with the bathwater, seemingly because the bathwater wasn't just the right temperature, i.e. didn't strike the right balance between ignorance and over-sensitivity for his liking. Furthermore in order to make this argument he caricatures the rationality movement as alarmist, but examination of its methods and texts leads to the opposite conclusion: it doesn't say that \"humanity is screwed\", but on the contrary that a thoughtful and bias-aware investigation of risk-adjusted costs is needed, hopefully leading to informed and effective mitigation of those risks. It's also a lazy critique, in that Pinker pokes fun at the term \"existential risk\" but doesn't engage here with a central problem in this area: the problem of how to account for very small probabilities of extremely bad outcomes. The point of rationalist thought-experiments is to expand the frame of reference so that we can make better choices, but Pinker doesn't want any fresh perspectives that might pop his progress-bubble. The chapter finishes with a review of nuclear proliferation, and concludes that we should continue steady progress towards reducing warhead stockpiles and seek to extend the 'no first use' doctrine. There is nothing much to note about this, except that he doesn't show that progress on this front really requires us to abstain from investigation of other risks. Overall this chapter misses his mark because it misrepresents its opponents in several ways and it doesn't engage with the central problems: the rationality movement is in fact very aware of the danger of cognitive bias in decision-making the superintelligence argument is that we must think about precautions to take, rather than being an attempt to halt all progress towards AGI (beneficial AGI, if achieved, would be valuable in combating other existential risks) defining AI goals and aligning AI values to our values are hard problems that need informed and widespread discussion what is at stake is potentially of astronomical importance and so it's inappropriate to apply our instinctive risk-assessment heuristics to it. Despite his initial claim that existential threats are \"problems to be solved\", all that Pinker's review of the field ends up doing is to suggest that -- aside from the issue of nuclear warhead decommissioning -- these problems can be expected to solve themselves in the normal course of events.","title":"Chapter 19 -- Existential Threats"},{"location":"research/20181231_enlightenment_now_pinker/#chapters-19-23","text":"TBC","title":"Chapters 19-23"},{"location":"research/reviews/","text":"Reviews December 2018 Superintelligence by Nick Bostrom (2014) 28th December 2018 A comprehensive review the existential risk posed by Artificial Intelligence and in particular by the possibility of a rapid intelligence explosion , and strategies to mitigate this risk. Concludes that although the timing and form of Superintelligence is uncertain, the risks are real and argues that urgent research into various problems is needed to allow us to take precautionary steps. Read more... AI Toy Control Problem - Stuart Armstrong (2017) 29th December 2018 A short animated YouTube video that illustrates the AI Control problem by using a simple AI agent which has a goal that is slightly misaligned with its creator's goal. A key point is that this shows an AI with sufficient planning skills can discover deceitful behaviour which evades safety provisions. if this improves realisation of its goal, without there being any implication of self-awareness or of independent will in the agent. Read more... Interview by Lex Fridman - Steven Pinker (2018) 30th December 2018 Pinker says that two kinds of existential threat have been discussed and he claims that both are incoherent, and furthermore that it's not worth spending too much time worrying about this because engineering culture is to test systems before using them. Read more... Enlightenment Now by Steven Pinker (2018) The Case for Reason, Science, Humanism and Progress 31st December 2018 Presents a positive vision of a new Age of Enlightenment in which human intelligence and culture is shown to be improving the lot of humanity. This vision is presented as a counterpoint to what Pinker sees as all too prevalent intellectual pessimism, but sadly it ends up becoming smug, unenlightened sneering at other contemporary intellectual discovery. Read more...","title":"Research"},{"location":"research/reviews/#reviews","text":"","title":"Reviews"},{"location":"research/reviews/#december-2018","text":"","title":"December 2018"},{"location":"research/reviews/#superintelligence-by-nick-bostrom-2014","text":"28th December 2018 A comprehensive review the existential risk posed by Artificial Intelligence and in particular by the possibility of a rapid intelligence explosion , and strategies to mitigate this risk. Concludes that although the timing and form of Superintelligence is uncertain, the risks are real and argues that urgent research into various problems is needed to allow us to take precautionary steps. Read more...","title":"Superintelligence by Nick Bostrom (2014)"},{"location":"research/reviews/#ai-toy-control-problem-stuart-armstrong-2017","text":"29th December 2018 A short animated YouTube video that illustrates the AI Control problem by using a simple AI agent which has a goal that is slightly misaligned with its creator's goal. A key point is that this shows an AI with sufficient planning skills can discover deceitful behaviour which evades safety provisions. if this improves realisation of its goal, without there being any implication of self-awareness or of independent will in the agent. Read more...","title":"AI Toy Control Problem - Stuart Armstrong (2017)"},{"location":"research/reviews/#interview-by-lex-fridman-steven-pinker-2018","text":"30th December 2018 Pinker says that two kinds of existential threat have been discussed and he claims that both are incoherent, and furthermore that it's not worth spending too much time worrying about this because engineering culture is to test systems before using them. Read more...","title":"Interview by Lex Fridman - Steven Pinker (2018)"},{"location":"research/reviews/#enlightenment-now-by-steven-pinker-2018","text":"The Case for Reason, Science, Humanism and Progress 31st December 2018 Presents a positive vision of a new Age of Enlightenment in which human intelligence and culture is shown to be improving the lot of humanity. This vision is presented as a counterpoint to what Pinker sees as all too prevalent intellectual pessimism, but sadly it ends up becoming smug, unenlightened sneering at other contemporary intellectual discovery. Read more...","title":"Enlightenment Now by Steven Pinker (2018)"}]}